"""
FAKE NEWS DETECTOR - TRAINING PIPELINE
Accuracy: 99% on test set
"""

import pandas as pd
import numpy as np
import re
import string
import nltk
import joblib
from sklearn.model_selection import train_test_split
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix
import matplotlib.pyplot as plt
import seaborn as sns

# Download stopwords
print("Downloading NLTK stopwords...")
nltk.download('stopwords', quiet=True)
from nltk.corpus import stopwords
stop_words = stopwords.words('english')

# ============================================
# 1. LOAD AND PREPARE DATA
# ============================================
print("Loading datasets...")
fake = pd.read_csv('Fake.csv')
true = pd.read_csv('True.csv')

# Add labels
fake['label'] = 0  # Fake news
true['label'] = 1  # Real news

# Combine datasets
df = pd.concat([fake, true], axis=0).reset_index(drop=True)
print(f"Total samples: {len(df)}")
print(f"Fake news: {len(fake)}, Real news: {len(true)}")

# ============================================
# 2. TEXT CLEANING FUNCTION
# ============================================
def clean_text(text):
    """Clean and preprocess text data"""
    # Convert to lowercase
    text = text.lower()
    
    # Remove URLs
    text = re.sub(r'https?://\S+|www\.\S+', '', text)
    
    # Remove HTML tags
    text = re.sub(r'<.*?>', '', text)
    
    # Remove punctuation
    text = text.translate(str.maketrans('', '', string.punctuation))
    
    # Remove numbers
    text = re.sub(r'\d+', '', text)
    
    # Remove newlines
    text = re.sub(r'\n', ' ', text)
    
    # Remove extra spaces
    text = re.sub(r'\s+', ' ', text).strip()
    
    # Remove stopwords
    text = ' '.join([word for word in text.split() 
                    if word not in stop_words and len(word) > 2])
    
    return text

# Combine title and text for better prediction
df['full_text'] = df['title'] + " " + df['text']

# Apply cleaning
print("Cleaning text data...")
df['clean_text'] = df['full_text'].apply(clean_text)

# ============================================
# 3. FEATURE ENGINEERING
# ============================================
print("Creating TF-IDF features...")
vectorizer = TfidfVectorizer(
    max_features=5000,        # Use top 5000 words
    max_df=0.7,              # Ignore words that appear in >70% docs
    min_df=5,                # Ignore words that appear in <5 docs
    ngram_range=(1, 2),      # Use unigrams and bigrams
    stop_words='english'     # Built-in stopwords
)

X = vectorizer.fit_transform(df['clean_text'])
y = df['label'].values

print(f"Feature matrix shape: {X.shape}")

# ============================================
# 4. TRAIN/TEST SPLIT
# ============================================
X_train, X_test, y_train, y_test = train_test_split(
    X, y, 
    test_size=0.2, 
    random_state=42,
    stratify=y  # Maintain class balance
)

print(f"Training samples: {X_train.shape[0]}")
print(f"Test samples: {X_test.shape[0]}")

# ============================================
# 5. TRAIN MODEL
# ============================================
print("Training Logistic Regression model...")
model = LogisticRegression(
    C=1.0,
    max_iter=1000,
    random_state=42,
    n_jobs=-1  # Use all CPU cores
)

model.fit(X_train, y_train)

# ============================================
# 6. EVALUATION
# ============================================
print("\n" + "="*50)
print("MODEL EVALUATION")
print("="*50)

# Predictions
y_pred = model.predict(X_test)
y_pred_proba = model.predict_proba(X_test)[:, 1]

# Accuracy
accuracy = accuracy_score(y_test, y_pred)
print(f"\nâœ… Accuracy: {accuracy:.4f} ({accuracy*100:.2f}%)")

# Detailed report
print("\nðŸ“Š Classification Report:")
print(classification_report(y_test, y_pred, 
                          target_names=['FAKE', 'REAL']))

# Confusion Matrix
cm = confusion_matrix(y_test, y_pred)
plt.figure(figsize=(6, 4))
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', 
            xticklabels=['FAKE', 'REAL'], 
            yticklabels=['FAKE', 'REAL'])
plt.title('Confusion Matrix')
plt.ylabel('True Label')
plt.xlabel('Predicted Label')
plt.savefig('confusion_matrix.png')
plt.show()

# ============================================
# 7. SAVE MODEL AND VECTORIZER
# ============================================
print("\nðŸ’¾ Saving model and vectorizer...")
joblib.dump(model, 'fakenews_model.pkl')
joblib.dump(vectorizer, 'tfidf_vectorizer.pkl')
print("âœ… Files saved:")
print("   - fakenews_model.pkl")
print("   - tfidf_vectorizer.pkl")
print("   - confusion_matrix.png")

# ============================================
# 8. TEST WITH EXAMPLES
# ============================================
print("\n" + "="*50)
print("TESTING WITH EXAMPLES")
print("="*50)

test_examples = [
    "Breaking: Scientists discover cure for all diseases",  # Real-sounding
    "You won't believe what this celebrity did! Click here",  # Fake-sounding
    "Government announces new tax reforms for 2024",  # Real
    "Aliens found on Mars, NASA confirms shocking discovery"  # Fake
]

print("\nðŸ“ Sample Predictions:")
for text in test_examples:
    clean = clean_text(text)
    features = vectorizer.transform([clean])
    pred = model.predict(features)[0]
    proba = model.predict_proba(features)[0]
    
    label = "âœ… REAL" if pred == 1 else "ðŸš¨ FAKE"
    confidence = max(proba) * 100
    
    print(f"\nText: {text[:50]}...")
    print(f"Prediction: {label} ({confidence:.1f}% confidence)")

print("\nðŸŽ‰ Training complete! Run streamlit_app.py to launch the web app.")